{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 5: Optimisation avec contraintes et calcul variationnel\n",
    "Tangi Migot et Paul Raynaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Manifest.toml`"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  ? \u001b[39mPDENLPModels\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Manifest.toml`"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  ? \u001b[39mPDENLPModels\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JuMP ─ v1.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Project.toml`\n",
      "  \u001b[90m[792afdf1] \u001b[39m\u001b[92m+ NLPModelsJuMP v0.12.5\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[90m[4076af6c] \u001b[39m\u001b[92m+ JuMP v1.20.0\u001b[39m\n",
      "  \u001b[90m[792afdf1] \u001b[39m\u001b[92m+ NLPModelsJuMP v0.12.5\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  ? \u001b[39mPDENLPModels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  ✓ \u001b[39m\u001b[90mJuMP\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m  ✓ \u001b[39mNLPModelsJuMP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2 dependencies successfully precompiled in 39 seconds. 115 already precompiled.\n",
      "  \u001b[33m2\u001b[39m dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n",
      "  \u001b[33m1\u001b[39m dependency failed but may be precompilable after restarting julia\n",
      "  \u001b[33m1\u001b[39m dependency had output during precompilation:\u001b[33m\n",
      "┌ \u001b[39mPDENLPModels\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition testargs(Gridap.Arrays.PosNegReindex{A, B} where B where A, Integer) in module Arrays at C:\\Users\\jgpal\\.julia\\packages\\Gridap\\EZQEK\\src\\Arrays\\PosNegReindex.jl:10 overwritten in module PDENLPModels at C:\\Users\\jgpal\\.julia\\packages\\PDENLPModels\\pW0Iv\\src\\PDENLPModels.jl:16.\u001b[33m\n",
      "│  \u001b[39mERROR: Method overwriting is not permitted during Module precompilation. Use `__precompile__(false)` to opt-out of precompilation.\u001b[33m\n",
      "└  \u001b[39m\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  ? \u001b[39mPDENLPModels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  ? \u001b[39mPDENLPModels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  ? \u001b[39mPDENLPModels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `C:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[90m[54578032] \u001b[39mADNLPModels v0.7.0\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[56d4f2e9] \u001b[39mGridap v0.15.5\n",
      "  \u001b[90m[7073ff75] \u001b[39mIJulia v1.24.2\n",
      "  \u001b[90m[ba0b0d4f] \u001b[39mKrylov v0.9.5\n",
      "  \u001b[90m[a4795742] \u001b[39mNLPModels v0.20.0\n",
      "  \u001b[90m[f4238b75] \u001b[39mNLPModelsIpopt v0.10.1\n",
      "  \u001b[90m[792afdf1] \u001b[39mNLPModelsJuMP v0.12.5\n",
      "  \u001b[90m[80da258d] \u001b[39mPDENLPModels v0.3.4\n",
      "  \u001b[90m[ff4d7338] \u001b[39mSolverCore v0.3.7\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra\n",
      "  \u001b[90m[56ddb016] \u001b[39mLogging\n",
      "  \u001b[90m[de0858da] \u001b[39mPrintf\n",
      "  \u001b[90m[8dfed614] \u001b[39mTest\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[33m⌅\u001b[39m have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n",
      "This is Ipopt version 3.14.14, running with linear solver MUMPS 5.6.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\") #Accède au fichier Project.toml\n",
    "Pkg.add(\"NLPModels\")\n",
    "Pkg.add(\"NLPModels\")\n",
    "Pkg.add(\"NLPModelsJuMP\")\n",
    "Pkg.add(\"NLPModelsIpopt\")\n",
    "Pkg.instantiate()\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Krylov, LinearAlgebra, Logging, NLPModels, NLPModelsIpopt, Printf, SolverCore, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PDENLPModels, Gridap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques commentaires en Julia\n",
    "\n",
    "### Les kwargs: choix optionnels\n",
    "\n",
    "Dans le projet du dernier labo, une des questions demandait d'ajouter une option pour utiliser la fonction `lsmr` ou `lsqr`. C'est le cas typique d'arguments optionnels:\n",
    "- On veut proposer un choix par défaut à l'utilisateur, par exemple `lsqr`;\n",
    "- On veut laisser la possibilité à l'utilisateur de changer;\n",
    "- On voudrait aussi pouvoir ajouter d'autres par la suite (sans avoir à tout modifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsol (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dsol(A, b, ϵ; solver :: Function = lsqr)\n",
    "    (d, stats) = solver(A, b, atol = ϵ)\n",
    "    return d\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter que l'on donne des valeurs par défaut aux arguments qui apparaissent après le `;`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Pénalité quadratique pour les ADNLPModels\n",
    "\n",
    "Dans cet exercice, on va étudier une version simple d'une méthode de pénalité quadratique pour les problèmes d'optimisation avec contraintes d'égalité.\n",
    "```math\n",
    "min f(x) s.à c(x) = 0.\n",
    "```\n",
    "Dans les labos précédents, on a déjà utilisé un NLPModel particulier, le ADNLPModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNLPModel - Model with automatic differentiation backend ADModelBackend{\n",
       "  ForwardDiffADGradient,\n",
       "  ForwardDiffADHvprod,\n",
       "  ForwardDiffADJprod,\n",
       "  ForwardDiffADJtprod,\n",
       "  ForwardDiffADJacobian,\n",
       "  ForwardDiffADHessian,\n",
       "  ForwardDiffADGHjvprod,\n",
       "}\n",
       "  Problem name: Generic\n",
       "   All variables: ████████████████████ 2      All constraints: ████████████████████ 1     \n",
       "            free: ████████████████████ 2                 free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ████████████████████ 1     \n",
       "          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            nnzh: (  0.00% sparsity)   3               linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                    nonlinear: ████████████████████ 1     \n",
       "                                                         nnzj: (  0.00% sparsity)   2     \n",
       "\n",
       "  Counters:\n",
       "             obj: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 grad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 cons: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "        cons_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0             cons_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 jcon: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jgrad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                  jac: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              jac_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         jac_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                jprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0            jprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "       jprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jtprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0           jtprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "      jtprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 hess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                hprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jhess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jhprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ADNLPModels, LinearAlgebra, Test\n",
    "fH(x) = (x[2]+x[1].^2-11)^2 + (x[1]+x[2].^2-7)^2\n",
    "x0H = [10., 20.]\n",
    "cH(x) = [x[1]-1]\n",
    "himmelblau = ADNLPModel(fH, x0H, cH, [0.], [0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: dans toute la suite de l'exercice on suppose que les bornes sur les contraintes `nlp.meta.lcon` et `nlp.meta.ucon` sont 0 pour simplifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Transformer un ADNLPModel en un problème pénalisé\n",
    "Coder la fonction `quad_penalty_adnlp` qui prend en entrée un ADNLPModel, et un paramètre ρ et qui retourne un nouveau ADNLPModel qui correspond au problème sans contrainte:\n",
    "$$\n",
    "\\min_x f(x) + \\frac{\\rho}{2}\\|c(x)\\|^2.\n",
    "$$\n",
    "Remarque: on peut accèder aux fonctions f et c par `NLPModels.obj()` et `NLPModels.cons()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quad_penalty_adnlp (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ADNLPModels #Pkg.add(\"ADNLPModels\")\n",
    "using NLPModels, NLPModelsIpopt\n",
    "\n",
    "\n",
    "\n",
    "function quad_penalty_adnlp(nlp :: ADNLPModel, ρ :: Real)\n",
    "    # TODO\n",
    "    f = x -> obj(nlp,x) + ρ/2 .* norm(cons(nlp,x))^2\n",
    "    nlp_quad = ADNLPModel(f, nlp.meta.x0)\n",
    "   return nlp_quad\n",
    "end\n",
    "\n",
    "function quad_penalty_adnlp(nlp :: MathOptNLPModel, ρ :: Real)\n",
    "    # TODO\n",
    "    f = x -> obj(nlp,x) + ρ/2 .* norm(cons(nlp,x))^2\n",
    "    nlp_quad = ADNLPModel(f, nlp.meta.x0)\n",
    "   return nlp_quad\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Ipopt version 3.14.14, running with linear solver MUMPS 5.6.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        3\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.7433050e+05 0.00e+00 1.00e+02  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  3.3498968e+04 0.00e+00 2.95e+01  -1.0 6.65e+00    -  1.00e+00 1.00e+00f  1\n",
      "   2  6.2387553e+03 0.00e+00 8.63e+00  -1.0 4.39e+00    -  1.00e+00 1.00e+00f  1\n",
      "   3  1.0981700e+03 0.00e+00 2.49e+00  -1.0 2.87e+00    -  1.00e+00 1.00e+00f  1\n",
      "   4  1.8219071e+02 0.00e+00 6.96e-01  -1.0 1.84e+00    -  1.00e+00 1.00e+00f  1\n",
      "   5  2.8534326e+01 0.00e+00 1.91e-01  -1.7 1.17e+00    -  1.00e+00 1.00e+00f  1\n",
      "   6  4.5704179e+00 0.00e+00 4.69e-02  -2.5 6.78e-01    -  1.00e+00 1.00e+00f  1\n",
      "   7  2.0605966e+00 0.00e+00 7.67e-03  -2.5 3.05e-01    -  1.00e+00 1.00e+00f  1\n",
      "   8  1.9683212e+00 0.00e+00 4.00e-04  -3.8 7.35e-02    -  1.00e+00 1.00e+00f  1\n",
      "   9  1.9680523e+00 0.00e+00 1.31e-06  -5.7 4.25e-03    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  1.9680523e+00 0.00e+00 1.41e-11  -8.6 1.40e-05    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 10\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   6.0633813587165422e-03    1.9680523214122152e+00\n",
      "Dual infeasibility......:   1.4118583374320126e-11    4.5826097916368261e-09\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Overall NLP error.......:   1.4118583374320126e-11    4.5826097916368261e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 11\n",
      "Number of objective gradient evaluations             = 11\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 10\n",
      "Total seconds in IPOPT                               = 1.534\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Execution stats: first-order stationary\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Faire des tests pour vérifier que ça fonctionne.\n",
    "\n",
    "himmelblau_quad = quad_penalty_adnlp(himmelblau, 1)\n",
    "@test himmelblau_quad.meta.ncon == 0\n",
    "@test obj(himmelblau_quad, zeros(2)) == 170.5\n",
    "\n",
    "stats = ipopt(himmelblau_quad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 2.9678888605539164\n",
       " 2.0186523682501782"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats.solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2})\n\nClosest candidates are:\n  (::Type{T})(::Real, !Matched::RoundingMode) where T<:AbstractFloat\n   @ Base rounding.jl:207\n  (::Type{T})(::T) where T<:Number\n   @ Core boot.jl:792\n  Float64(!Matched::IrrationalConstants.Log2π)\n   @ IrrationalConstants C:\\Users\\jgpal\\.julia\\packages\\IrrationalConstants\\vp5v4\\src\\macro.jl:112\n  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2})\n",
      "\n",
      "Closest candidates are:\n",
      "  (::Type{T})(::Real, !Matched::RoundingMode) where T<:AbstractFloat\n",
      "   @ Base rounding.jl:207\n",
      "  (::Type{T})(::T) where T<:Number\n",
      "   @ Core boot.jl:792\n",
      "  Float64(!Matched::IrrationalConstants.Log2π)\n",
      "   @ IrrationalConstants C:\\Users\\jgpal\\.julia\\packages\\IrrationalConstants\\vp5v4\\src\\macro.jl:112\n",
      "  ...\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] convert(::Type{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2})\n",
      "    @ Base .\\number.jl:7\n",
      "  [2] setindex!(A::Vector{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}, i1::Int64)\n",
      "    @ Base .\\array.jl:1021\n",
      "  [3] _forward_eval(f::MathOptInterface.Nonlinear.ReverseAD._FunctionStorage, d::MathOptInterface.Nonlinear.ReverseAD.NLPEvaluator, x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}})\n",
      "    @ MathOptInterface.Nonlinear.ReverseAD C:\\Users\\jgpal\\.julia\\packages\\MathOptInterface\\nEHaN\\src\\Nonlinear\\ReverseAD\\reverse_mode.jl:96\n",
      "  [4] _reverse_mode(d::MathOptInterface.Nonlinear.ReverseAD.NLPEvaluator, x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}})\n",
      "    @ MathOptInterface.Nonlinear.ReverseAD C:\\Users\\jgpal\\.julia\\packages\\MathOptInterface\\nEHaN\\src\\Nonlinear\\ReverseAD\\reverse_mode.jl:42\n",
      "  [5] eval_objective\n",
      "    @ MathOptInterface.Nonlinear.ReverseAD C:\\Users\\jgpal\\.julia\\packages\\MathOptInterface\\nEHaN\\src\\Nonlinear\\ReverseAD\\mathoptinterface_api.jl:177 [inlined]\n",
      "  [6] eval_objective(evaluator::MathOptInterface.Nonlinear.Evaluator{MathOptInterface.Nonlinear.ReverseAD.NLPEvaluator}, x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}})\n",
      "    @ MathOptInterface.Nonlinear C:\\Users\\jgpal\\.julia\\packages\\MathOptInterface\\nEHaN\\src\\Nonlinear\\evaluator.jl:129\n",
      "  [7] obj(nlp::MathOptNLPModel, x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}})\n",
      "    @ NLPModelsJuMP C:\\Users\\jgpal\\.julia\\packages\\NLPModelsJuMP\\1KSlI\\src\\moi_nlp_model.jl:81\n",
      "  [8] (::var\"#122#123\"{MathOptNLPModel, Int64})(x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}})\n",
      "    @ Main c:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Lab5-notebook.ipynb:15\n",
      "  [9] vector_mode_dual_eval!\n",
      "    @ C:\\Users\\jgpal\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\apiutils.jl:24 [inlined]\n",
      " [10] vector_mode_gradient!(result::Vector{Float64}, f::var\"#122#123\"{MathOptNLPModel, Int64}, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}}})\n",
      "    @ ForwardDiff C:\\Users\\jgpal\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\gradient.jl:96\n",
      " [11] gradient!\n",
      "    @ ForwardDiff C:\\Users\\jgpal\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\gradient.jl:37 [inlined]\n",
      " [12] gradient!(result::Vector{Float64}, f::var\"#122#123\"{MathOptNLPModel, Int64}, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#122#123\"{MathOptNLPModel, Int64}, Float64}, Float64, 2}}})\n",
      "    @ ForwardDiff C:\\Users\\jgpal\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\gradient.jl:35\n",
      " [13] gradient!(adbackend::ADNLPModels.ForwardDiffADGradient, g::Vector{Float64}, f::Function, x::Vector{Float64})\n",
      "    @ ADNLPModels C:\\Users\\jgpal\\.julia\\packages\\ADNLPModels\\Q4sHr\\src\\forward.jl:24\n",
      " [14] grad!(nlp::ADNLPModel{Float64, Vector{Float64}, Vector{Int64}}, x::Vector{Float64}, g::Vector{Float64})\n",
      "    @ ADNLPModels C:\\Users\\jgpal\\.julia\\packages\\ADNLPModels\\Q4sHr\\src\\nlp.jl:542\n",
      " [15] (::NLPModelsIpopt.var\"#eval_grad_f#3\"{ADNLPModel{Float64, Vector{Float64}, Vector{Int64}}})(x::Vector{Float64}, g::Vector{Float64})\n",
      "    @ NLPModelsIpopt C:\\Users\\jgpal\\.julia\\packages\\NLPModelsIpopt\\WBABa\\src\\NLPModelsIpopt.jl:95\n",
      " [16] _Eval_Grad_F_CB(n::Int32, x_ptr::Ptr{Float64}, ::Int32, grad_f::Ptr{Float64}, user_data::Ptr{Nothing})\n",
      "    @ Ipopt C:\\Users\\jgpal\\.julia\\packages\\Ipopt\\YDBAD\\src\\C_wrapper.jl:54\n",
      " [17] IpoptSolve(prob::IpoptProblem)\n",
      "    @ Ipopt C:\\Users\\jgpal\\.julia\\packages\\Ipopt\\YDBAD\\src\\C_wrapper.jl:442\n",
      " [18] solve!(solver::IpoptSolver, nlp::ADNLPModel{Float64, Vector{Float64}, Vector{Int64}}, stats::GenericExecutionStats{Float64, Vector{Float64}, Vector{Float64}, Any}; callback::Function, kwargs::@Kwargs{})\n",
      "    @ NLPModelsIpopt C:\\Users\\jgpal\\.julia\\packages\\NLPModelsIpopt\\WBABa\\src\\NLPModelsIpopt.jl:240\n",
      " [19] solve!\n",
      "    @ NLPModelsIpopt C:\\Users\\jgpal\\.julia\\packages\\NLPModelsIpopt\\WBABa\\src\\NLPModelsIpopt.jl:161 [inlined]\n",
      " [20] #ipopt#6\n",
      "    @ NLPModelsIpopt C:\\Users\\jgpal\\.julia\\packages\\NLPModelsIpopt\\WBABa\\src\\NLPModelsIpopt.jl:158 [inlined]\n",
      " [21] ipopt(nlp::ADNLPModel{Float64, Vector{Float64}, Vector{Int64}})\n",
      "    @ NLPModelsIpopt C:\\Users\\jgpal\\.julia\\packages\\NLPModelsIpopt\\WBABa\\src\\NLPModelsIpopt.jl:155\n",
      " [22] top-level scope\n",
      "    @ c:\\Users\\jgpal\\OneDrive\\Documents\\GitHub\\MTH8408-Hiv24\\lab5_JP\\Lab5-notebook.ipynb:19"
     ]
    }
   ],
   "source": [
    "#Ajouter au moins un autre test similaire avec des contraintes.\n",
    "using JuMP \n",
    "using NLPModels, LinearAlgebra, NLPModelsJuMP, NLPModelsIpopt\n",
    "\n",
    "m = Model(Ipopt.Optimizer)\n",
    "@variable(m, x[1:2])\n",
    "x0 = [10. ,20.]\n",
    "set_start_value.(x, x0) \n",
    "@NLobjective(m, Min, sum((x[i] for i in 1:2)))#sum((x[i]^2) for i in 1:2))\n",
    "@constraint(m, x[1]==2)\n",
    "\n",
    "nlp = MathOptNLPModel(m)\n",
    "\n",
    "\n",
    "\n",
    "new_quad = quad_penalty_adnlp(nlp, 1)\n",
    "@test new_quad.meta.ncon == 0\n",
    "\n",
    "stats = ipopt(new_quad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter un test au cas ou `nlp.meta.lcon` ou `nlp.meta.ucon` ont des composantes differentes de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: KKT\n",
    "Coder une fonction `KKT_eq_constraint(nlp :: AbstractNLPModel, x, λ)` qui vérifie si le point `x` avec multiplicateur de Lagrange `λ` satisfait les conditions KKT d'un problème avec contraintes d'égalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KKT_eq_constraint (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function KKT_eq_constraint(nlp :: AbstractNLPModel, x, λ)\n",
    "   # TODO\n",
    "   constr = cons(nlp,x)\n",
    "   ∇c= grad(constr,x)\n",
    "   ∇f = grad(nlp,x)\n",
    "   if (∇f == dot(λ,∇c)) & (constr == zeros(nlp.nvar))\n",
    "      kkt_bool= true\n",
    "   else\n",
    "      kkt_bool= false\n",
    "   end\n",
    "   return kkt_bool\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: méthode de pénalité quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModelsIpopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function quad_penalty(nlp      :: AbstractNLPModel,\n",
    "                      x        :: AbstractVector; \n",
    "                      ϵ        :: AbstractFloat = 1e-3,\n",
    "                      η        :: AbstractFloat = 1e6, \n",
    "                      σ        :: AbstractFloat = 2.0,\n",
    "                      max_eval :: Int = 1_000, \n",
    "                      max_time :: AbstractFloat = 60.,\n",
    "                      max_iter :: Int = typemax(Int64)\n",
    "                      )\n",
    "    ##### Initialiser cx et gx au point x;\n",
    "    cx = cons(nlp,x) # Initialiser la violation des contraintes\n",
    "    gx = grad(nlp, x) # Initialiser le gradient\n",
    "    ######################################################\n",
    "    normcx = normcx_old = norm(cx)\n",
    "\n",
    "    ρ = 1.\n",
    "\n",
    "    iter = 0    \n",
    "\n",
    "    el_time = 0.0\n",
    "    tired   = neval_cons(nlp) > max_eval || el_time > max_time\n",
    "    status  = :unknown\n",
    "\n",
    "    start_time = time()\n",
    "    too_small  = false\n",
    "    normdual   = norm(gx) #exceptionnellement on ne va pas vérifier toute l'optimalité au début.\n",
    "    optimal    = max(normcx, normdual) ≤ ϵ\n",
    "    \n",
    "    nlp_quad   = quad_penalty_adnlp(nlp, ρ)\n",
    "\n",
    "    @info log_header([:iter, :nf, :primal, :status, :nd, :Δ],\n",
    "    [Int, Int, Float64, String, Float64, Float64],\n",
    "    hdr_override=Dict(:nf => \"#F\", :primal => \"‖F(x)‖\", :nd => \"‖d‖\"))\n",
    "\n",
    "    while !(optimal || tired || too_small)\n",
    "\n",
    "        #Appeler Ipopt pour résoudre le problème pénalisé en partant du point x0 = x.\n",
    "        #utiliser l'option print_level = 0 pour enlever les affichages d'ipopt.\n",
    "        stats = ipopt(nlp_quad) #...\n",
    "        ################################################\n",
    "      \n",
    "        if stats.status == :first_order\n",
    "            ###### Mettre à jour cx avec la solution renvoyé par Ipopt \n",
    "            \"\"\"\n",
    "            https://jso.dev/SolverCore.jl/dev/reference/#SolverCore.GenericExecutionStats\n",
    "            \"\"\"\n",
    "            x = stats.solution #TODO\n",
    "            cx = cons(nlp,x) #TODO\n",
    "            ##########################################################\n",
    "            normcx_old = normcx\n",
    "            normcx = norm(cx)\n",
    "        end\n",
    "        \n",
    "        if normcx_old > 0.95 * normcx\n",
    "            ρ *= σ\n",
    "        end\n",
    "\n",
    "        @info log_row(Any[iter, neval_cons(nlp), normcx, stats.status])\n",
    "        \n",
    "        nlp_quad   = quad_penalty_adnlp(nlp, ρ)\n",
    "\n",
    "        el_time      = time() - start_time\n",
    "        iter   += 1\n",
    "        many_evals   = neval_cons(nlp) > max_eval\n",
    "        iter_limit   = iter > max_iter\n",
    "        tired        = many_evals || el_time > max_time || iter_limit || ρ ≥ η\n",
    "        ##### Utiliser la réalisabilité dual renvoyé par Ipopt pour `normdual`\n",
    "        normdual     = stats.dual_feasibility # TODO\n",
    "        ###################################################################\n",
    "        optimal      = max(normcx, normdual) ≤ ϵ\n",
    "    end\n",
    "\n",
    "    status = if optimal \n",
    "        :first_order\n",
    "    elseif tired\n",
    "        if neval_cons(nlp) > max_eval\n",
    "            :max_eval\n",
    "        elseif el_time > max_time\n",
    "            :max_time\n",
    "        elseif iter > max_iter\n",
    "            :max_iter\n",
    "        else\n",
    "            :unknown_tired\n",
    "        end\n",
    "    elseif too_small\n",
    "        :stalled\n",
    "    else\n",
    "        :unknown\n",
    "    end\n",
    "\n",
    "    return GenericExecutionStats(nlp, status = status, solution = x,\n",
    "                                 objective = obj(nlp, x),\n",
    "                                 primal_feas = normcx,\n",
    "                                 dual_feas = normdual,\n",
    "                                 iter = iter, \n",
    "                                 elapsed_time = el_time,\n",
    "                                 solver_specific = Dict(:penalty => ρ))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire des tests pour vérifier que ça fonctionne.\n",
    "stats = quad_penalty(himmelblau, x0H)\n",
    "@test stats.status == :first_order\n",
    "@test stats.solution ≈ [1.0008083416169895, 2.709969135758311] atol=1e-2\n",
    "@test norm(cons(himmelblau, stats.solution)) ≈ 0. atol=1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que la solution rendue vérifie les conditions KKT avec la fonction de la question précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "KKT_eq_constraint(nlp = himmelblau, stats.solution, λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fichier de tests à demander.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: Calcul Variationnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, on considère le problème de calcul variationnel suivant:\n",
    "$$\n",
    "\\min \\int_0^1 (\\dot{x}(t)^2+2x(t)^2)e^t dt, \\quad x(0)=0, x(1)=e - e^{-2}\n",
    "$$\n",
    "\n",
    "modélisé avec `PDENLPModels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cv_model(n :: Int)\n",
    "\n",
    "  domain = (0,1) # set the domain\n",
    "  partition = n\n",
    "  model = CartesianDiscreteModel(domain,partition) # set discretization\n",
    "    \n",
    "  labels = get_face_labeling(model)\n",
    "  add_tag_from_tags!(labels,\"diri1\",[2])\n",
    "  add_tag_from_tags!(labels,\"diri0\",[1]) # boundary conditions\n",
    "\n",
    "  order=1\n",
    "  valuetype=Float64\n",
    "  reffe = ReferenceFE(lagrangian, valuetype, order)\n",
    "  V0 = TestFESpace(model, reffe; conformity=:H1, dirichlet_tags=[\"diri0\",\"diri1\"])\n",
    "  U = TrialFESpace(V0,[0., exp(1)-exp(-2)])\n",
    "\n",
    "  trian = Triangulation(model)\n",
    "  degree = 2\n",
    "  dΩ = Measure(trian,degree) # integration machinery\n",
    "\n",
    "  # Our objective function\n",
    "  w(x) = exp(x[1])\n",
    "  function f(y)\n",
    "    ∫((∇(y)⊙∇(y) + 2 * y * y) * w) * dΩ\n",
    "  end\n",
    "\n",
    "  xin = zeros(Gridap.FESpaces.num_free_dofs(U))\n",
    "  nlp = GridapPDENLPModel(xin, f, trian, U, V0)\n",
    "  return nlp\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Résoudre\n",
    "Résoudre le NLPModel généré par la fonction `cv_model` pour `n = 16` avec `ipopt` et afficher la solution (attention la solution rendue ne contient pas les valeurs aux bords qu'il faut rajouter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Convergence en `n`\n",
    "Afficher sur un même graphique la solution obtenue par `ipopt` pour plusieurs valeurs de `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Comparer à la solution exacte\n",
    "\n",
    "La solution exacte est $x(t)=e^t - e^{-2t}$ et la valeur optimale est $e^3 - 2e^{-3}+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
